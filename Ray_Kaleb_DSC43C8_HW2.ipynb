{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Kaleb Ray - Big Data HW2"
   ],
   "metadata": {
    "id": "Pk3QkQIjXHFU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DNN MINIST and CIFAR10"
   ],
   "metadata": {
    "id": "_RRHeq71Yf_5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Packages"
   ],
   "metadata": {
    "id": "lOMStAiklOQT"
   }
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sYOGxM9QW_xk",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, log_loss\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load MINIST\n",
    "\n",
    "Since CIFAR10 is by default an 80-20 train-test split, we will use an 80-20 split on the MINIST data since the split is not clarified in the assignment."
   ],
   "metadata": {
    "id": "gqADglOMY7i7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version = 1, as_frame = False)\n",
    "# Separate the features and target\n",
    "X, y = mnist.data, mnist.target\n",
    "# Convert target to numeric\n",
    "y = y.astype(int)\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "x_train_minist, x_test_minist, y_train_minist, y_test_minist = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HE0BJJW2hiyL",
    "outputId": "78660706-931f-4af7-93cd-33db7988d0aa"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of X: (70000, 784)\n",
      "Shape of y: (70000,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load CIFAR10"
   ],
   "metadata": {
    "id": "Mat2XJhZY9s9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load CIFAR-10 dataset\n",
    "(x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar) = cifar10.load_data()\n",
    "# Print summary of CIFAR-10 data\n",
    "print(\"Training data shape:\", x_train_cifar.shape)  # (50000, 32, 32, 3)\n",
    "print(\"Training labels shape:\", y_train_cifar.shape)  # (50000, 1)\n",
    "print(\"Testing data shape:\", x_test_cifar.shape)  # (10000, 32, 32, 3)\n",
    "print(\"Testing labels shape:\", y_test_cifar.shape)  # (10000, 1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8JpWWELYr1a",
    "outputId": "5ec88c34-bb40-4b6b-ec92-4db07074ff93"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001B[1m170498071/170498071\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 0us/step\n",
      "Training data shape: (50000, 32, 32, 3)\n",
      "Training labels shape: (50000, 1)\n",
      "Testing data shape: (10000, 32, 32, 3)\n",
      "Testing labels shape: (10000, 1)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## D-NN for MINIST and CIFAR10"
   ],
   "metadata": {
    "id": "hymdXC45ZJO3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# To normalize minist, implement a form of MinMax normalization by dividing by 255\n",
    "# Scales values from [0, 1]\n",
    "x_train_minist = x_train_minist / 255.0\n",
    "x_test_minist = x_test_minist / 255.0\n",
    "\n",
    "# For CIFAR, we use z-score standardization\n",
    "# First, retrieve the mean and standard deviation of each RGB channel\n",
    "mean_cifar = np.mean(x_train_cifar, axis = (0, 1, 2))\n",
    "std_cifar = np.std(x_train_cifar, axis = (0, 1, 2))\n",
    "# Divide by 255 for data, mean, and std to scale values from [0, 1], scaling\n",
    "### mean and std accordingly with the data\n",
    "x_train_cifar_norm = x_train_cifar / 255.0\n",
    "x_test_cifar_norm = x_test_cifar / 255.0\n",
    "mean_cifar_norm = mean_cifar / 255.0\n",
    "std_cifar_norm = std_cifar / 255.0\n",
    "x_train_cifar = (x_train_cifar_norm - mean_cifar_norm) / std_cifar_norm\n",
    "x_test_cifar = (x_test_cifar_norm - mean_cifar_norm) / std_cifar_norm"
   ],
   "metadata": {
    "id": "hRi2w0HhZRi4"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Metrics"
   ],
   "metadata": {
    "id": "mjtqRdaNfGND"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_measure(true_label, predicted_label):\n",
    "\n",
    "  t_idx = (true_label == predicted_label)\n",
    "  f_idx = np.logical_not(t_idx)\n",
    "  p_idx = (true_label > 0)\n",
    "  n_idx = np.logical_not(p_idx)\n",
    "  tp = np.sum(np.logical_and(t_idx, p_idx))\n",
    "\n",
    "  tn = np.sum(np.logical_and(t_idx, n_idx))\n",
    "\n",
    "  fp = np.sum(n_idx) - tn\n",
    "  fn = np.sum(p_idx) - tp\n",
    "  tp_fp_tn_fn_list = []\n",
    "\n",
    "  with np.errstate(divide = 'ignore'):\n",
    "    sen = (1.0 * tp) / (tp + fn)\n",
    "  with np.errstate(divide = 'ignore'):\n",
    "    spec = (1.0 * tn) / (tn + fp)\n",
    "  with np.errstate(divide = 'ignore'):\n",
    "    f1 = tp / (tp + 0.5 * (fp + fn))\n",
    "\n",
    "  acc = (tp + tn) * 1.0 / (tp + fp + tn + fn)\n",
    "\n",
    "  d = np.log2(1 + acc) + np.log2(1 + (sen + spec) / 2)\n",
    "  f1_micro = f1_score(true_label, predicted_label, average = 'micro')\n",
    "  f1_macro = f1_score(true_label, predicted_label, average = 'macro')\n",
    "\n",
    "  ans = []\n",
    "  ans.append(acc)\n",
    "  ans.append(sen)\n",
    "  ans.append(spec)\n",
    "  ans.append(f1_micro)\n",
    "  ans.append(f1_macro)\n",
    "  ans.append(d)\n",
    "\n",
    "  return ans"
   ],
   "metadata": {
    "id": "BPdcvuCgfFjv"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def model_metrics(true, pred):\n",
    "\n",
    "  ans = compute_measure(true, pred)\n",
    "  print(\"Accuracy is {0:4f}\".format(ans[0]))\n",
    "  print(\"Sensitivity is {0:4f}\".format(ans[1]))\n",
    "  print(\"Specificity is {0:4f}\".format(ans[2]))\n",
    "  print(\"F1-Score Micro is {0:4f}\".format(ans[3]))\n",
    "  print(\"F1-Score Macro is {0:4f}\".format(ans[4]))\n",
    "  print(\"Diagnostic Index is {0:4f}\".format(ans[5]))"
   ],
   "metadata": {
    "id": "-NGZq0RqfJj1"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MINIST"
   ],
   "metadata": {
    "id": "KZhgcI-SZPBi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**SGD**"
   ],
   "metadata": {
    "id": "9-KgMI15YxcW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "clf_minist_sgd = MLPClassifier(\n",
    "    hidden_layer_sizes = (200, 100, 50),\n",
    "    max_iter = 100,\n",
    "    activation = 'relu',\n",
    "    learning_rate = 'adaptive',\n",
    "    solver = 'sgd',\n",
    "    learning_rate_init = 0.01,\n",
    "    random_state = 0,\n",
    "    warm_start = True\n",
    ")\n",
    "\n",
    "# Log Loss\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "n_epoch = 100\n",
    "\n",
    "for _ in range(n_epoch):\n",
    "  clf_minist_sgd.partial_fit(x_train_minist, y_train_minist, classes = np.unique(y))\n",
    "  train_losses.append(log_loss(y_train_minist, clf_minist_sgd.predict_proba(x_train_minist)))\n",
    "  test_losses.append(log_loss(y_test_minist, clf_minist_sgd.predict_proba(x_test_minist)))\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_minist = clf_minist_sgd.predict(x_test_minist)"
   ],
   "metadata": {
    "id": "XuCuWCA4g9R4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot losses for MINIST\n",
    "fig = plt.figure(figsize = (7, 3))\n",
    "plt.plot(train_losses, label = 'Training Loss')\n",
    "plt.plot(test_losses, 'b-', label = 'Test Loss')\n",
    "plt.title('Training and Test Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('on')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "9YacMq16usqg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for vanishing / exploding gradients\n",
    "final_train_loss = train_losses[-1]\n",
    "if np.isnan(final_train_loss) or final_train_loss > 1e3:\n",
    "  print(\"Warning: Exploding gradients detected!\")\n",
    "# no improvement after 10 epochs\n",
    "elif train_losses[10] - final_train_loss < 1e-5:\n",
    "  print(\"Warning: Potential vanishing gradients detected!\")\n",
    "else:\n",
    "  print(\"No vanishing or exploding gradients detected!\")"
   ],
   "metadata": {
    "id": "D0-LXZUUvVGU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Confusion Matrix for MINIST\n",
    "cm = confusion_matrix(y_test_minist, y_pred_minist)\n",
    "print(cm)"
   ],
   "metadata": {
    "id": "JIG-4s_zs3Dm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Metrics for MINIST\n",
    "model_metrics(y_test_minist, y_pred_minist)"
   ],
   "metadata": {
    "id": "hM69nMzJoP02"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate eta for train and test loss curves\n",
    "eta_train = 1 - (np.std(train_losses)/(np.mean(train_losses) + (1e-10)))**2\n",
    "print(\"Eta of Train Curve: \", eta_train)\n",
    "eta_test = 1 - (np.std(test_losses)/(np.mean(test_losses) + (1e-10)))**2\n",
    "print(\"Eta of Test Curve: \", eta_test)\n",
    "eta_train_test = eta_test / eta_train\n",
    "print(\"Eta Test over Train: \", eta_train_test)"
   ],
   "metadata": {
    "id": "uGGusYiCXcYf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**ADAM**"
   ],
   "metadata": {
    "id": "ETg3NqNTY5du"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "clf_minist_adam = MLPClassifier(\n",
    "    hidden_layer_sizes = (200, 100, 50),\n",
    "    max_iter = 100,\n",
    "    activation = 'relu',\n",
    "    learning_rate = 'adaptive',\n",
    "    solver = 'adam',\n",
    "    learning_rate_init = 0.01,\n",
    "    random_state = 0,\n",
    "    warm_start = True\n",
    ")\n",
    "\n",
    "# Log Loss\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "n_epoch = 100\n",
    "\n",
    "for _ in range(n_epoch):\n",
    "  clf_minist_adam.partial_fit(x_train_minist, y_train_minist, classes = np.unique(y))\n",
    "  train_losses.append(log_loss(y_train_minist, clf_minist_adam.predict_proba(x_train_minist)))\n",
    "  test_losses.append(log_loss(y_test_minist, clf_minist_adam.predict_proba(x_test_minist)))\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_minist = clf_minist_adam.predict(x_test_minist)"
   ],
   "metadata": {
    "id": "mwtqE4UaY68R"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot losses for MINIST\n",
    "fig = plt.figure(figsize = (7, 3))\n",
    "plt.plot(train_losses, label = 'Training Loss')\n",
    "plt.plot(test_losses, 'b-', label = 'Test Loss')\n",
    "plt.title('Training and Test Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('on')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "h_jnXjtaZeei"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for vanishing / exploding gradients\n",
    "final_train_loss = train_losses[-1]\n",
    "if np.isnan(final_train_loss) or final_train_loss > 1e3:\n",
    "  print(\"Warning: Exploding gradients detected!\")\n",
    "# no improvement after 10 epochs\n",
    "elif train_losses[10] - final_train_loss < 1e-5:\n",
    "  print(\"Warning: Potential vanishing gradients detected!\")\n",
    "else:\n",
    "  print(\"No vanishing or exploding gradients detected!\")"
   ],
   "metadata": {
    "id": "l7pBFHp0Ze65"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Confusion Matrix for MINIST\n",
    "cm = confusion_matrix(y_test_minist, y_pred_minist)\n",
    "print(cm)"
   ],
   "metadata": {
    "id": "O7xXpPl6ZjNh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Metrics for MINIST\n",
    "model_metrics(y_test_minist, y_pred_minist)"
   ],
   "metadata": {
    "id": "Lf4mg7IxZjqo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate eta for train and test loss curves\n",
    "eta_train = 1 - (np.std(train_losses)/(np.mean(train_losses) + (1e-10)))**2\n",
    "print(\"Eta of Train Curve: \", eta_train)\n",
    "eta_test = 1 - (np.std(test_losses)/(np.mean(test_losses) + (1e-10)))**2\n",
    "print(\"Eta of Test Curve: \", eta_test)\n",
    "eta_train_test = eta_test / eta_train\n",
    "print(\"Eta Test over Train: \", eta_train_test)"
   ],
   "metadata": {
    "id": "DiX_ZG53Zlqw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CIFAR10"
   ],
   "metadata": {
    "id": "77FTw6xWtBsq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Must flatten the data before MLP\n",
    "x_train_cifar = x_train_cifar.reshape(x_train_cifar.shape[0], -1)\n",
    "x_test_cifar = x_test_cifar.reshape(x_test_cifar.shape[0], -1)"
   ],
   "metadata": {
    "id": "RJLml7pMXTgr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**SGD**"
   ],
   "metadata": {
    "id": "WEBoMvIuZ9RZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "clf_cifar_sgd = MLPClassifier(\n",
    "    hidden_layer_sizes = (200, 100, 50),\n",
    "    max_iter = 100,\n",
    "    activation = 'relu',\n",
    "    learning_rate = 'adaptive',\n",
    "    solver = 'sgd',\n",
    "    learning_rate_init = 0.01,\n",
    "    random_state = 0,\n",
    "    warm_start = True\n",
    ")\n",
    "\n",
    "# Log Loss\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "n_epoch = 100\n",
    "\n",
    "for _ in range(n_epoch):\n",
    "  clf_cifar_sgd.partial_fit(x_train_cifar, y_train_cifar, classes = np.unique(y_train_cifar))\n",
    "  train_losses.append(log_loss(y_train_cifar, clf_cifar_sgd.predict_proba(x_train_cifar)))\n",
    "  test_losses.append(log_loss(y_test_cifar, clf_cifar_sgd.predict_proba(x_test_cifar)))\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_cifar = clf_cifar_sgd.predict(x_test_cifar)"
   ],
   "metadata": {
    "id": "BoTP13djtSsP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot losses for CIFAR\n",
    "fig = plt.figure(figsize = (7, 3))\n",
    "plt.plot(train_losses, label = 'Training Loss')\n",
    "plt.plot(test_losses, 'b-', label = 'Test Loss')\n",
    "plt.title('Training and Test Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('on')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "9u24YBUBu0Oh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for vanishing / exploding gradients\n",
    "final_train_loss = train_losses[-1]\n",
    "if np.isnan(final_train_loss) or final_train_loss > 1e3:\n",
    "  print(\"Warning: Exploding gradients detected!\")\n",
    "# no improvement after 10 epochs\n",
    "elif train_losses[10] - final_train_loss < 1e-5:\n",
    "  print(\"Warning: Potential vanishing gradients detected!\")\n",
    "else:\n",
    "  print(\"No vanishing or exploding gradients detected!\")"
   ],
   "metadata": {
    "id": "mr_7JAKAvQiT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Confusion Matrix for MINIST\n",
    "cm = confusion_matrix(y_test_cifar, y_pred_cifar)\n",
    "print(cm)"
   ],
   "metadata": {
    "id": "_ZSJKc7NugDp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Metrics for MINIST\n",
    "model_metrics(y_test_cifar, y_pred_cifar)"
   ],
   "metadata": {
    "id": "N6cgmNBxughC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate eta for train and test loss curves\n",
    "eta_train = 1 - (np.std(train_losses)/(np.mean(train_losses) + (1e-10)))**2\n",
    "print(\"Eta of Train Curve: \", eta_train)\n",
    "eta_test = 1 - (np.std(test_losses)/(np.mean(test_losses) + (1e-10)))**2\n",
    "print(\"Eta of Test Curve: \", eta_test)\n",
    "eta_train_test = eta_test / eta_train\n",
    "print(\"Eta Test over Train: \", eta_train_test)"
   ],
   "metadata": {
    "id": "oUJFg69vXXRu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**ADAM**"
   ],
   "metadata": {
    "id": "OBC2ssXTaGsg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "clf_cifar_adam = MLPClassifier(\n",
    "    hidden_layer_sizes = (200, 100, 50),\n",
    "    max_iter = 100,\n",
    "    activation = 'relu',\n",
    "    learning_rate = 'adaptive',\n",
    "    solver = 'adam',\n",
    "    learning_rate_init = 0.01,\n",
    "    random_state = 0,\n",
    "    warm_start = True\n",
    ")\n",
    "\n",
    "# Log Loss\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "n_epoch = 100\n",
    "\n",
    "for _ in range(n_epoch):\n",
    "  clf_cifar_adam.partial_fit(x_train_cifar, y_train_cifar, classes = np.unique(y_train_cifar))\n",
    "  train_losses.append(log_loss(y_train_cifar, clf_cifar_adam.predict_proba(x_train_cifar)))\n",
    "  test_losses.append(log_loss(y_test_cifar, clf_cifar_adam.predict_proba(x_test_cifar)))\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_cifar = clf_cifar_adam.predict(x_test_cifar)"
   ],
   "metadata": {
    "id": "NXOpHKv9aH2b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot losses for CIFAR\n",
    "fig = plt.figure(figsize = (7, 3))\n",
    "plt.plot(train_losses, label = 'Training Loss')\n",
    "plt.plot(test_losses, 'b-', label = 'Test Loss')\n",
    "plt.title('Training and Test Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('on')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "RLHRa4WAaQRX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for vanishing / exploding gradients\n",
    "final_train_loss = train_losses[-1]\n",
    "if np.isnan(final_train_loss) or final_train_loss > 1e3:\n",
    "  print(\"Warning: Exploding gradients detected!\")\n",
    "# no improvement after 10 epochs\n",
    "elif train_losses[10] - final_train_loss < 1e-5:\n",
    "  print(\"Warning: Potential vanishing gradients detected!\")\n",
    "else:\n",
    "  print(\"No vanishing or exploding gradients detected!\")"
   ],
   "metadata": {
    "id": "XBEu7U0DaT98"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Confusion Matrix for MINIST\n",
    "cm = confusion_matrix(y_test_cifar, y_pred_cifar)\n",
    "print(cm)"
   ],
   "metadata": {
    "id": "9PCxWC5NaUbg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Metrics for MINIST\n",
    "model_metrics(y_test_cifar, y_pred_cifar)"
   ],
   "metadata": {
    "id": "sKafASX6aWVl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate eta for train and test loss curves\n",
    "eta_train = 1 - (np.std(train_losses)/(np.mean(train_losses) + (1e-10)))**2\n",
    "print(\"Eta of Train Curve: \", eta_train)\n",
    "eta_test = 1 - (np.std(test_losses)/(np.mean(test_losses) + (1e-10)))**2\n",
    "print(\"Eta of Test Curve: \", eta_test)\n",
    "eta_train_test = eta_test / eta_train\n",
    "print(\"Eta Test over Train: \", eta_train_test)"
   ],
   "metadata": {
    "id": "qrlhVD1KaYK0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PCA MINIST"
   ],
   "metadata": {
    "id": "xGDParqybSVL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def doPCA_DNN(train_data, test_data, need_exp_var):\n",
    "  pca = PCA(n_components = train_data.shape[1])\n",
    "  data_pca = pca.fit_transform(train_data)\n",
    "  exp_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "  n_comp = np.argmax(exp_var >= need_exp_var) + 1\n",
    "  train_pca = data_pca[:, :n_comp]\n",
    "  test_pca = pca.transform(test_data)[:, :n_comp]\n",
    "  return train_pca, test_pca"
   ],
   "metadata": {
    "id": "3-WVPd-RbVWh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**SGD**"
   ],
   "metadata": {
    "id": "aEuJ-JwMwNQU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_minist_pca, test_minist_pca = doPCA_DNN(x_train_minist, x_test_minist)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "n_epoch = 100\n",
    "\n",
    "for _ in range(n_epoch):\n",
    "  clf_minist_sgd.partial_fit(train_minist_pca, y_train_minist, classes = np.unique(y))\n",
    "  train_losses.append(log_loss(y_train_minist, clf_minist_sgd.predict_proba(train_minist_pca)))\n",
    "  test_losses.append(log_loss(y_test_minist, clf_minist_sgd.predict_proba(test_minist_pca)))\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_minist = clf_minist_sgd.predict(test_minist_pca)"
   ],
   "metadata": {
    "id": "5Sf2uPZauLc1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot losses for MINIST\n",
    "fig = plt.figure(figsize = (7, 3))\n",
    "plt.plot(train_losses, label = 'Training Loss')\n",
    "plt.plot(test_losses, 'b-', label = 'Test Loss')\n",
    "plt.title('Training and Test Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('on')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "QqQ5aYFlv2yZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for vanishing / exploding gradients\n",
    "final_train_loss = train_losses[-1]\n",
    "if np.isnan(final_train_loss) or final_train_loss > 1e3:\n",
    "  print(\"Warning: Exploding gradients detected!\")\n",
    "# no improvement after 10 epochs\n",
    "elif train_losses[10] - final_train_loss < 1e-5:\n",
    "  print(\"Warning: Potential vanishing gradients detected!\")\n",
    "else:\n",
    "  print(\"No vanishing or exploding gradients detected!\")"
   ],
   "metadata": {
    "id": "hFdFMb3gv3tF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Confusion Matrix for MINIST\n",
    "cm = confusion_matrix(y_test_minist, y_pred_minist)\n",
    "print(cm)"
   ],
   "metadata": {
    "id": "Kwef50O6v77W"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Metrics for MINIST\n",
    "model_metrics(y_test_minist, y_pred_minist)"
   ],
   "metadata": {
    "id": "3uO2f6MDwAvM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate eta for train and test loss curves\n",
    "eta_train = 1 - (np.std(train_losses)/(np.mean(train_losses) + (1e-10)))**2\n",
    "print(\"Eta of Train Curve: \", eta_train)\n",
    "eta_test = 1 - (np.std(test_losses)/(np.mean(test_losses) + (1e-10)))**2\n",
    "print(\"Eta of Test Curve: \", eta_test)\n",
    "eta_train_test = eta_test / eta_train\n",
    "print(\"Eta Test over Train: \", eta_train_test)"
   ],
   "metadata": {
    "id": "qKeyLbVfwIa0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**ADAM**"
   ],
   "metadata": {
    "id": "sdccyOZuwRgi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "n_epoch = 100\n",
    "\n",
    "for _ in range(n_epoch):\n",
    "  clf_minist_adam.partial_fit(train_minist_pca, y_train_minist, classes = np.unique(y))\n",
    "  train_losses.append(log_loss(y_train_minist, clf_minist_adam.predict_proba(train_minist_pca)))\n",
    "  test_losses.append(log_loss(y_test_minist, clf_minist_adam.predict_proba(test_minist_pca)))\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_minist = clf_minist_adam.predict(test_minist_pca)"
   ],
   "metadata": {
    "id": "D3DJwxQKwS-M"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot losses for MINIST\n",
    "fig = plt.figure(figsize = (7, 3))\n",
    "plt.plot(train_losses, label = 'Training Loss')\n",
    "plt.plot(test_losses, 'b-', label = 'Test Loss')\n",
    "plt.title('Training and Test Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('on')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "Q1Xilh5DwbxL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for vanishing / exploding gradients\n",
    "final_train_loss = train_losses[-1]\n",
    "if np.isnan(final_train_loss) or final_train_loss > 1e3:\n",
    "  print(\"Warning: Exploding gradients detected!\")\n",
    "# no improvement after 10 epochs\n",
    "elif train_losses[10] - final_train_loss < 1e-5:\n",
    "  print(\"Warning: Potential vanishing gradients detected!\")\n",
    "else:\n",
    "  print(\"No vanishing or exploding gradients detected!\")"
   ],
   "metadata": {
    "id": "WNnWA6fZwevh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Confusion Matrix for MINIST\n",
    "cm = confusion_matrix(y_test_minist, y_pred_minist)\n",
    "print(cm)"
   ],
   "metadata": {
    "id": "8Lrd5ET5whzC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Metrics for MINIST\n",
    "model_metrics(y_test_minist, y_pred_minist)"
   ],
   "metadata": {
    "id": "vy7Sf4KKwkNN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate eta for train and test loss curves\n",
    "eta_train = 1 - (np.std(train_losses)/(np.mean(train_losses) + (1e-10)))**2\n",
    "print(\"Eta of Train Curve: \", eta_train)\n",
    "eta_test = 1 - (np.std(test_losses)/(np.mean(test_losses) + (1e-10)))**2\n",
    "print(\"Eta of Test Curve: \", eta_test)\n",
    "eta_train_test = eta_test / eta_train\n",
    "print(\"Eta Test over Train: \", eta_train_test)"
   ],
   "metadata": {
    "id": "Fwn2UEFtwmuT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PCA CIFAR"
   ],
   "metadata": {
    "id": "mWNEgRBowte3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**SGD**"
   ],
   "metadata": {
    "id": "cl_HeIZswvyY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_cifar_pca, test_cifar_pca = doPCA_DNN(x_train_cifar, x_test_cifar)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "n_epoch = 100\n",
    "\n",
    "for _ in range(n_epoch):\n",
    "  clf_cifar_sgd.partial_fit(train_cifar_pca, y_train_cifar, classes = np.unique(y_train_cifar))\n",
    "  train_losses.append(log_loss(y_train_cifar, clf_cifar_sgd.predict_proba(train_cifar_pca)))\n",
    "  test_losses.append(log_loss(y_test_cifar, clf_cifar_sgd.predict_proba(test_cifar_pca)))\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_cifar = clf_cifar_sgd.predict(test_cifar_pca)"
   ],
   "metadata": {
    "id": "4QwT_HSQwyhp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot losses for CIFAR\n",
    "fig = plt.figure(figsize = (7, 3))\n",
    "plt.plot(train_losses, label = 'Training Loss')\n",
    "plt.plot(test_losses, 'b-', label = 'Test Loss')\n",
    "plt.title('Training and Test Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('on')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "2sjvLpjvxTLP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for vanishing / exploding gradients\n",
    "final_train_loss = train_losses[-1]\n",
    "if np.isnan(final_train_loss) or final_train_loss > 1e3:\n",
    "  print(\"Warning: Exploding gradients detected!\")\n",
    "# no improvement after 10 epochs\n",
    "elif train_losses[10] - final_train_loss < 1e-5:\n",
    "  print(\"Warning: Potential vanishing gradients detected!\")\n",
    "else:\n",
    "  print(\"No vanishing or exploding gradients detected!\")"
   ],
   "metadata": {
    "id": "SEO69RSYxVZ1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Confusion Matrix for CIFAR\n",
    "cm = confusion_matrix(y_test_cifar, y_pred_cifar)\n",
    "print(cm)"
   ],
   "metadata": {
    "id": "Kuhh1_P7xX1q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Metrics for CIFAR\n",
    "model_metrics(y_test_cifar, y_pred_cifar)"
   ],
   "metadata": {
    "id": "c2JFWdCkxd8j"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate eta for train and test loss curves\n",
    "eta_train = 1 - (np.std(train_losses)/(np.mean(train_losses) + (1e-10)))**2\n",
    "print(\"Eta of Train Curve: \", eta_train)\n",
    "eta_test = 1 - (np.std(test_losses)/(np.mean(test_losses) + (1e-10)))**2\n",
    "print(\"Eta of Test Curve: \", eta_test)\n",
    "eta_train_test = eta_test / eta_train\n",
    "print(\"Eta Test over Train: \", eta_train_test)"
   ],
   "metadata": {
    "id": "_upKBUX4xiof"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**ADAM**"
   ],
   "metadata": {
    "id": "tg0O_OUWwxHn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "n_epoch = 100\n",
    "\n",
    "for _ in range(n_epoch):\n",
    "  clf_cifar_adam.partial_fit(train_cifar_pca, y_train_cifar, classes = np.unique(y_train_cifar))\n",
    "  train_losses.append(log_loss(y_train_cifar, clf_cifar_adam.predict_proba(train_cifar_pca)))\n",
    "  test_losses.append(log_loss(y_test_cifar, clf_cifar_adam.predict_proba(test_cifar_pca)))\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_cifar = clf_cifar_adam.predict(test_cifar_pca)"
   ],
   "metadata": {
    "id": "h0Rg1x7gxmxe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot losses for CIFAR\n",
    "fig = plt.figure(figsize = (7, 3))\n",
    "plt.plot(train_losses, label = 'Training Loss')\n",
    "plt.plot(test_losses, 'b-', label = 'Test Loss')\n",
    "plt.title('Training and Test Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('on')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "bWW92KLuxvr4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for vanishing / exploding gradients\n",
    "final_train_loss = train_losses[-1]\n",
    "if np.isnan(final_train_loss) or final_train_loss > 1e3:\n",
    "  print(\"Warning: Exploding gradients detected!\")\n",
    "# no improvement after 10 epochs\n",
    "elif train_losses[10] - final_train_loss < 1e-5:\n",
    "  print(\"Warning: Potential vanishing gradients detected!\")\n",
    "else:\n",
    "  print(\"No vanishing or exploding gradients detected!\")"
   ],
   "metadata": {
    "id": "zmJkoHTNxyhP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Confusion Matrix for CIFAR\n",
    "cm = confusion_matrix(y_test_cifar, y_pred_cifar)\n",
    "print(cm)"
   ],
   "metadata": {
    "id": "oggmpXDrx0r8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Metrics for CIFAR\n",
    "model_metrics(y_test_cifar, y_pred_cifar)"
   ],
   "metadata": {
    "id": "1kM0F1Jrx244"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate eta for train and test loss curves\n",
    "eta_train = 1 - (np.std(train_losses)/(np.mean(train_losses) + (1e-10)))**2\n",
    "print(\"Eta of Train Curve: \", eta_train)\n",
    "eta_test = 1 - (np.std(test_losses)/(np.mean(test_losses) + (1e-10)))**2\n",
    "print(\"Eta of Test Curve: \", eta_test)\n",
    "eta_train_test = eta_test / eta_train\n",
    "print(\"Eta Test over Train: \", eta_train_test)"
   ],
   "metadata": {
    "id": "AIIVPz4sx6Wc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PCA + T-SNE MINIST"
   ],
   "metadata": {
    "id": "rGetZTEnyJsE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def doTSNE_DNN(train_data, test_data):\n",
    "  tsne = TSNE(n_components = 2, random_state = 0)\n",
    "  train_tsne = tsne.fit_transform(train_data)\n",
    "  test_tsne = tsne.transform(test_data)\n",
    "  return train_tsne, test_tsne"
   ],
   "metadata": {
    "id": "7EIb_MDCyVz4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_minist_tsne, test_minist_tsne = doTSNE_DNN(train_minist_pca, test_minist_pca)"
   ],
   "metadata": {
    "id": "SYLUbhacy1Py"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**SGD**"
   ],
   "metadata": {
    "id": "YnKactoByOhe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "n_epoch = 100\n",
    "\n",
    "for _ in range(n_epoch):\n",
    "  clf_minist_sgd.partial_fit(train_minist_tsne, y_train_minist, classes = np.unique(y))\n",
    "  train_losses.append(log_loss(y_train_minist, clf_minist_sgd.predict_proba(train_minist_tsne)))\n",
    "  test_losses.append(log_loss(y_test_minist, clf_minist_sgd.predict_proba(test_minist_tsne)))\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_minist = clf_minist_sgd.predict(test_minist_tsne)"
   ],
   "metadata": {
    "id": "SXAtQU6azAVD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot losses for MINIST\n",
    "fig = plt.figure(figsize = (7, 3))\n",
    "plt.plot(train_losses, label = 'Training Loss')\n",
    "plt.plot(test_losses, 'b-', label = 'Test Loss')\n",
    "plt.title('Training and Test Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('on')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "Y6h-bqdAzLtS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for vanishing / exploding gradients\n",
    "final_train_loss = train_losses[-1]\n",
    "if np.isnan(final_train_loss) or final_train_loss > 1e3:\n",
    "  print(\"Warning: Exploding gradients detected!\")\n",
    "# no improvement after 10 epochs\n",
    "elif train_losses[10] - final_train_loss < 1e-5:\n",
    "  print(\"Warning: Potential vanishing gradients detected!\")\n",
    "else:\n",
    "  print(\"No vanishing or exploding gradients detected!\")"
   ],
   "metadata": {
    "id": "RGTHDIbKzRZC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Confusion Matrix for MINIST\n",
    "cm = confusion_matrix(y_test_minist, y_pred_minist)\n",
    "print(cm)"
   ],
   "metadata": {
    "id": "bCJJBEVrzT0Z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Metrics for MINIST\n",
    "model_metrics(y_test_minist, y_pred_minist)"
   ],
   "metadata": {
    "id": "JeZ4ONTezaR5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate eta for train and test loss curves\n",
    "eta_train = 1 - (np.std(train_losses)/(np.mean(train_losses) + (1e-10)))**2\n",
    "print(\"Eta of Train Curve: \", eta_train)\n",
    "eta_test = 1 - (np.std(test_losses)/(np.mean(test_losses) + (1e-10)))**2\n",
    "print(\"Eta of Test Curve: \", eta_test)\n",
    "eta_train_test = eta_test / eta_train\n",
    "print(\"Eta Test over Train: \", eta_train_test)"
   ],
   "metadata": {
    "id": "e_NrppXlze8q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**ADAM**"
   ],
   "metadata": {
    "id": "S3pShcPyyQSK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "n_epoch = 100\n",
    "\n",
    "for _ in range(n_epoch):\n",
    "  clf_minist_adam.partial_fit(train_minist_tsne, y_train_minist, classes = np.unique(y))\n",
    "  train_losses.append(log_loss(y_train_minist, clf_minist_adam.predict_proba(train_minist_tsne)))\n",
    "  test_losses.append(log_loss(y_test_minist, clf_minist_adam.predict_proba(test_minist_tsne)))\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_minist = clf_minist_adam.predict(test_minist_tsne)"
   ],
   "metadata": {
    "id": "cPxcky8azjDO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot losses for MINIST\n",
    "fig = plt.figure(figsize = (7, 3))\n",
    "plt.plot(train_losses, label = 'Training Loss')\n",
    "plt.plot(test_losses, 'b-', label = 'Test Loss')\n",
    "plt.title('Training and Test Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('on')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "21VKoaBTzpvC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for vanishing / exploding gradients\n",
    "final_train_loss = train_losses[-1]\n",
    "if np.isnan(final_train_loss) or final_train_loss > 1e3:\n",
    "  print(\"Warning: Exploding gradients detected!\")\n",
    "# no improvement after 10 epochs\n",
    "elif train_losses[10] - final_train_loss < 1e-5:\n",
    "  print(\"Warning: Potential vanishing gradients detected!\")\n",
    "else:\n",
    "  print(\"No vanishing or exploding gradients detected!\")"
   ],
   "metadata": {
    "id": "IqnwuI77zsWe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Confusion Matrix for MINIST\n",
    "cm = confusion_matrix(y_test_minist, y_pred_minist)\n",
    "print(cm)"
   ],
   "metadata": {
    "id": "xoUreB61zuVk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Metrics for MINIST\n",
    "model_metrics(y_test_minist, y_pred_minist)"
   ],
   "metadata": {
    "id": "Qlqm8jCFzwVC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate eta for train and test loss curves\n",
    "eta_train = 1 - (np.std(train_losses)/(np.mean(train_losses) + (1e-10)))**2\n",
    "print(\"Eta of Train Curve: \", eta_train)\n",
    "eta_test = 1 - (np.std(test_losses)/(np.mean(test_losses) + (1e-10)))**2\n",
    "print(\"Eta of Test Curve: \", eta_test)\n",
    "eta_train_test = eta_test / eta_train\n",
    "print(\"Eta Test over Train: \", eta_train_test)"
   ],
   "metadata": {
    "id": "d0kyQgUNz06-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PCA + T-SNE CIFAR"
   ],
   "metadata": {
    "id": "tZLguEM8yMJG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_cifar_tsne, test_cifar_tsne = doTSNE_DNN(train_cifar_pca, test_cifar_pca)"
   ],
   "metadata": {
    "id": "53acmYgbz3uG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**SGD**"
   ],
   "metadata": {
    "id": "jUGjd9-UyRmM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "n_epoch = 100\n",
    "\n",
    "for _ in range(n_epoch):\n",
    "  clf_cifar_sgd.partial_fit(train_cifar_tsne, y_train_cifar, classes = np.unique(y_train_cifar))\n",
    "  train_losses.append(log_loss(y_train_cifar, clf_cifar_sgd.predict_proba(train_cifar_tsne)))\n",
    "  test_losses.append(log_loss(y_test_cifar, clf_cifar_sgd.predict_proba(test_cifar_tsne)))\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_cifar = clf_cifar_sgd.predict(test_cifar_tsne)"
   ],
   "metadata": {
    "id": "EB-In85Kz_fT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot losses for MINIST\n",
    "fig = plt.figure(figsize = (7, 3))\n",
    "plt.plot(train_losses, label = 'Training Loss')\n",
    "plt.plot(test_losses, 'b-', label = 'Test Loss')\n",
    "plt.title('Training and Test Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('on')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "biIvgb7i0lBZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for vanishing / exploding gradients\n",
    "final_train_loss = train_losses[-1]\n",
    "if np.isnan(final_train_loss) or final_train_loss > 1e3:\n",
    "  print(\"Warning: Exploding gradients detected!\")\n",
    "# no improvement after 10 epochs\n",
    "elif train_losses[10] - final_train_loss < 1e-5:\n",
    "  print(\"Warning: Potential vanishing gradients detected!\")\n",
    "else:\n",
    "  print(\"No vanishing or exploding gradients detected!\")"
   ],
   "metadata": {
    "id": "YnMj3YxM0p55"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Confusion Matrix for CIFAR\n",
    "cm = confusion_matrix(y_test_cifar, y_pred_cifar)\n",
    "print(cm)"
   ],
   "metadata": {
    "id": "vN7jbduM0tih"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Metrics for CIFAR\n",
    "model_metrics(y_test_cifar, y_pred_cifar)"
   ],
   "metadata": {
    "id": "A4_8qp6o0zLS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate eta for train and test loss curves\n",
    "eta_train = 1 - (np.std(train_losses)/(np.mean(train_losses) + (1e-10)))**2\n",
    "print(\"Eta of Train Curve: \", eta_train)\n",
    "eta_test = 1 - (np.std(test_losses)/(np.mean(test_losses) + (1e-10)))**2\n",
    "print(\"Eta of Test Curve: \", eta_test)\n",
    "eta_train_test = eta_test / eta_train\n",
    "print(\"Eta Test over Train: \", eta_train_test)"
   ],
   "metadata": {
    "id": "4Q-aZULz07fs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**ADAM**"
   ],
   "metadata": {
    "id": "1pS18S_vyTdl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "n_epoch = 100\n",
    "\n",
    "for _ in range(n_epoch):\n",
    "  clf_cifar_adam.partial_fit(train_cifar_tsne, y_train_cifar, classes = np.unique(y_train_cifar))\n",
    "  train_losses.append(log_loss(y_train_cifar, clf_cifar_adam.predict_proba(train_cifar_tsne)))\n",
    "  test_losses.append(log_loss(y_test_cifar, clf_cifar_adam.predict_proba(test_cifar_tsne)))\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_cifar = clf_cifar_adam.predict(test_cifar_tsne)"
   ],
   "metadata": {
    "id": "gRpI7Icn0dSe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot losses for MINIST\n",
    "fig = plt.figure(figsize = (7, 3))\n",
    "plt.plot(train_losses, label = 'Training Loss')\n",
    "plt.plot(test_losses, 'b-', label = 'Test Loss')\n",
    "plt.title('Training and Test Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('on')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "bt5ndjQ30mPL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for vanishing / exploding gradients\n",
    "final_train_loss = train_losses[-1]\n",
    "if np.isnan(final_train_loss) or final_train_loss > 1e3:\n",
    "  print(\"Warning: Exploding gradients detected!\")\n",
    "# no improvement after 10 epochs\n",
    "elif train_losses[10] - final_train_loss < 1e-5:\n",
    "  print(\"Warning: Potential vanishing gradients detected!\")\n",
    "else:\n",
    "  print(\"No vanishing or exploding gradients detected!\")"
   ],
   "metadata": {
    "id": "_v82NhHm0rJw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Confusion Matrix for CIFAR\n",
    "cm = confusion_matrix(y_test_cifar, y_pred_cifar)\n",
    "print(cm)"
   ],
   "metadata": {
    "id": "PS_omjfx0x9H"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Metrics for CIFAR\n",
    "model_metrics(y_test_cifar, y_pred_cifar)"
   ],
   "metadata": {
    "id": "P9_UkFGR0ykA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate eta for train and test loss curves\n",
    "eta_train = 1 - (np.std(train_losses)/(np.mean(train_losses) + (1e-10)))**2\n",
    "print(\"Eta of Train Curve: \", eta_train)\n",
    "eta_test = 1 - (np.std(test_losses)/(np.mean(test_losses) + (1e-10)))**2\n",
    "print(\"Eta of Test Curve: \", eta_test)\n",
    "eta_train_test = eta_test / eta_train\n",
    "print(\"Eta Test over Train: \", eta_train_test)"
   ],
   "metadata": {
    "id": "WrApEfZK08lL"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
